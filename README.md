# ğŸ¬ IMDB Movie Ratings Sentiment Analysis  
*Using Logistic Regression, Random Forest, and Naive Bayes*  

## ğŸ“Œ Project Overview  
This project performs **sentiment analysis** on IMDB movie reviews to classify them as **positive or negative**. The model is trained using **text embeddings (GloVe, Word2Vec, TF-IDF)** and evaluated using **Logistic Regression, Random Forest, and Naive Bayes**.  

## ğŸ“‚ Dataset  
- **Source**: [IMDB Movie Reviews Dataset]([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis))  
- **Size**: 50,000 labeled movie reviews  
- **Classes**:  
  - **Positive (1)** â€“ Favorable movie reviews (Positive reviews) 
  - **Negative (0)** â€“ Unfavorable movie reviews (Negative reviews)  

## ğŸ” Objective  
- **Clean and preprocess text data** (remove stopwords, lemmatization, tokenization).  
- Extract meaningful **word embeddings** using **GloVe, Word2Vec, and TF-IDF**.  
- Build and compare multiple machine learning models for **sentiment classification**.  
- Evaluate models using **accuracy, precision, recall, and F1-score**.  

## ğŸ› ï¸ Technologies Used  
- **Python**: NumPy, Pandas, Scikit-learn, NLTK, Matplotlib, Seaborn  
- **Machine Learning**: Logistic Regression, Random Forest, NaÃ¯ve Bayes  
- **Word Embeddings**: GloVe, Word2Vec, TF-IDF  
- **Jupyter Notebook** for model training and analysis  

## ğŸ—ï¸ Model Training Process  
1. **Text Preprocessing**  
   - Removed punctuation, stopwords, and special characters  
   - Tokenized and lemmatized words using NLTK  
   - Converted text into numerical features using **TF-IDF, Word2Vec, and GloVe**  

2. **Feature Extraction**  
   - **TF-IDF:** Converts text into numerical vectors based on word importance  
   - **Word2Vec:** Captures word meanings in vector space  
   - **GloVe:** Generates pre-trained word vectors for contextual understanding  

3. **Model Training & Evaluation**  
   - **Logistic Regression** (Baseline Model)  
   - **Random Forest** (Tree-based Model)  
   - **NaÃ¯ve Bayes** (Probabilistic Model)  

## ğŸ“ˆ Model Performance  
| Model              | Precision | Recall | F1-score |  
|--------------------|-----------|--------|-----------|  
| **Logistic Regression** | 89% | 89% | 89% |  
| **Random Forest**  85% | 85% | 85% |  
| **NaÃ¯ve Bayes** | 80% | 89% | 79% |  

- **Logistic Regression performed best** due to its effectiveness with TF-IDF features.  
- **Random Forest struggled with sparse vector representations**.  
- **NaÃ¯ve Bayes was efficient but had lower recall**.  

## ğŸ”¥ Key Insights  
âœ… **TF-IDF performed better with Logistic Regression compared to word embeddings**.  
âœ… **GloVe and Word2Vec embeddings worked well with deep learning models but less with ML classifiers**.  
âœ… **Random Forest had feature importance but struggled with text data sparsity**.  

## ğŸš€ Future Improvements  
ğŸ”¹ Experiment with **LSTMs and Transformers (BERT, RoBERTa)** for improved accuracy.  
ğŸ”¹ Implement **hyperparameter tuning** to optimize model performance.  
ğŸ”¹ Build a **Flask/Streamlit web app** for real-time movie review sentiment analysis.  
